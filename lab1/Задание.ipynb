{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAt-K2qgcIou"
   },
   "source": [
    "# Нейронная сеть с двумя слоями\n",
    "\n",
    "Постройте нейронную сеть с двумя слоями и обучите её решать задачу классификации.\n",
    "\n",
    "**После выполнения этого задания вы сможете:**\n",
    "\n",
    "- Реализовать нейронную сеть с двумя слоями для задачи классификации\n",
    "- Реализовать прямое распространение с помощью матричного умножения\n",
    "- Выполнить обратное распространение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Содержание\n",
    "\n",
    "- [ 1 - Задача классификации](#1)\n",
    "- [ 2 - Модель нейронной сети с двумя слоями](#2)\n",
    "  - [ 2.1 - Модель нейронной сети с двумя слоями для одного обучающего примера](#2.1)\n",
    "  - [ 2.2 - Модель нейронной сети с двумя слоями для нескольких обучающих примеров](#2.2)\n",
    "  - [ 2.3 - Функция потерь и обучение](#2.3)\n",
    "  - [ 2.4 - Набор данных](#2.4)\n",
    "  - [ 2.5 - Определение функции активации](#2.5)\n",
    "    - [ Упражнение 1](#ex01)\n",
    "- [ 3 - Реализация модели нейронной сети с двумя слоями](#3)\n",
    "  - [ 3.1 - Определение структуры нейронной сети](#3.1)\n",
    "    - [ Упражнение 2](#ex02)\n",
    "  - [ 3.2 - Инициализация параметров модели](#3.2)\n",
    "    - [ Упражнение 3](#ex03)\n",
    "  - [ 3.3 - Цикл](#3.3)\n",
    "    - [ Упражнение 4](#ex04)\n",
    "    - [ Упражнение 5](#ex05)\n",
    "    - [ Упражнение 6](#ex06)\n",
    "  - [ 3.4 - Интеграция частей 3.1, 3.2 и 3.3 в nn_model()](#3.4)\n",
    "    - [ Упражнение 7](#ex07)\n",
    "    - [ Упражнение 8](#ex08)\n",
    "- [ 4 - Другой набор данных](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пакеты\n",
    "\n",
    "Сначала импортируем все пакеты, которые нам понадобятся в этом задании."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "# Функция для создания набора данных.\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Результат графических команд отображается напрямую в Jupyter блокноте.\n",
    "%matplotlib inline \n",
    "\n",
    "# Установим начальное значение, чтобы результаты были вопроизводимыми.\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим модульные тесты, определенные для этого блокнота."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import w3_unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Задача классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранее мы обучали нейронную сеть с одним перцептроном, выполняя прямое и обратное распространение. Этой простой структуры модели было достаточно для решения \"линейной\" задачи классификации — нахождении прямой в плоскости, которая будет служить границей решения для разделения двух классов.\n",
    "\n",
    "Представьте, что теперь у вас более сложная задача: у вас все еще есть два класса, но одной прямой будет недостаточно, чтобы их разделить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "xmin, xmax = -0.2, 1.4\n",
    "x_line = np.arange(xmin, xmax, 0.1)\n",
    "# Точки данных (наблюдения) из двух классов.\n",
    "ax.scatter(0, 0, color=\"r\")\n",
    "ax.scatter(0, 1, color=\"b\")\n",
    "ax.scatter(1, 0, color=\"b\")\n",
    "ax.scatter(1, 1, color=\"r\")\n",
    "ax.set_xlim([xmin, xmax])\n",
    "ax.set_ylim([-0.1, 1.1])\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "# Пример прямых, которые могут быть использованы в качестве границы решения для разделения двух классов.\n",
    "ax.plot(x_line, -1 * x_line + 1.5, color=\"black\")\n",
    "ax.plot(x_line, -1 * x_line + 0.5, color=\"black\")\n",
    "plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта логика может присутствовать во многих приложениях. Например, если мы обучим модель предсказывать, следует ли нам покупать дом, зная его размер и год постройки. Большой новый дом не будет доступным, в то время как маленький старый дом не будет стоить покупки. Таким образом, мы можем быть заинтересованы либо в большом старом доме, либо в маленьком новом доме.\n",
    "\n",
    "Одного перцептрона недостаточно, чтобы решить такую задачу классификации. Давайте посмотрим, как можем адаптировать эту модель, чтобы найти решение.\n",
    "\n",
    "На графике выше две прямых могут служить границей решения. Наша интуиция подсказывает, что также следует увеличить число перцептронов. И это верно. Нам нужно будет подавать данные (координаты $x_1$, $x_2$) на два узла отдельно, а затем объединить их с другим узлом для принятия решения.\n",
    "\n",
    "Теперь давайте разберемся с деталями, построим и обучим нашу многослойную нейронную сеть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Модель нейронной сети с двумя слоями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "### 2.1 - Модель нейронной сети с двумя слоями для одного обучающего примера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/nn_model_2_layers.png\" style=\"width:1000px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Входные и выходные слои нейронной сети такие же, как для модели с одним перцептроном, но теперь между ними есть **скрытый слой**. Обучающие примеры $x^{(i)}=\\begin{bmatrix}x_1^{(i)} \\\\ x_2^{(i)}\\end{bmatrix}$ из входного слоя размера $n_x = 2$ сначала подаются на скрытый слой размера $n_h = 2$. Они одновременно подаются на первый перцептрон с весами $W_1^{[1]}=\\begin{bmatrix}w_{1,1}^{[1]} & w_{2,1}^{[1]}\\end{bmatrix}$, смещение $b_1^{[1]}$; и на второй перцептрон с весами $W_2^{[1]}=\\begin{bmatrix}w_{1,2}^{[1]} & w_{2,2}^{[1]}\\end{bmatrix}$, смещение $b_2^{[1]}$. Целое число в квадратных скобках $^{[1]}$ указывает номер слоя, поскольку теперь есть два слоя с их собственными параметрами и выводами, которые необходимо различать.\n",
    "\n",
    "\\begin{align}\n",
    "z_1^{[1](i)} &= w_{1,1}^{[1]} x_1^{(i)} + w_{2,1}^{[1]} x_2^{(i)} + b_1^{[1]} = W_1^{[1]}x^{(i)} + b_1^{[1]},\\\\\n",
    "z_2^{[1](i)} &= w_{1,2}^{[1]} x_1^{(i)} + w_{2,2}^{[1]} x_2^{(i)} + b_2^{[1]} = W_2^{[1]}x^{(i)} + b_2^{[1]}.\\tag{1}\n",
    "\\end{align}\n",
    "\n",
    "Эти выражения для одного обучающего примера $x^{(i)}$ могут быть переписаны в матричной форме:\n",
    "\n",
    "$$z^{[1](i)} = W^{[1]} x^{(i)} + b^{[1]},\\tag{2}$$\n",
    "\n",
    "где \n",
    "\n",
    "&emsp; &emsp; $z^{[1](i)} = \\begin{bmatrix}z_1^{[1](i)} \\\\ z_2^{[1](i)}\\end{bmatrix}$ — вектор размера $\\left(n_h \\times 1\\right) = \\left(2 \\times 1\\right)$; \n",
    "\n",
    "&emsp; &emsp; $W^{[1]} = \\begin{bmatrix}W_1^{[1]} \\\\ W_2^{[1]}\\end{bmatrix} = \n",
    "\\begin{bmatrix}w_{1,1}^{[1]} & w_{2,1}^{[1]} \\\\ w_{1,2}^{[1]} & w_{2,2}^{[1]}\\end{bmatrix}$ — матрица размера $\\left(n_h \\times n_x\\right) = \\left(2 \\times 2\\right)$;\n",
    "\n",
    "&emsp; &emsp; $b^{[1]} = \\begin{bmatrix}b_1^{[1]} \\\\ b_2^{[1]}\\end{bmatrix}$ — вектор размера $\\left(n_h \\times 1\\right) = \\left(2 \\times 1\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее необходимо применить функцию активации скрытого слоя для каждого из элементов вектора $z^{[1](i)}$. Здесь можно использовать различные функции активации, и в этой модели мы возьмем сигмоидальную функцию $\\sigma\\left(x\\right) = \\frac{1}{1 + e^{-x}}$. Помните, что её производная равна $\\frac{d\\sigma}{dx} = \\sigma\\left(x\\right)\\left(1-\\sigma\\left(x\\right)\\right)$. Выход скрытого слоя — это вектор размера $\\left(n_h \\times 1\\right) = \\left(2 \\times 1\\right)$:\n",
    "\n",
    "$$a^{[1](i)} = \\sigma\\left(z^{[1](i)}\\right) = \n",
    "\\begin{bmatrix}\\sigma\\left(z_1^{[1](i)}\\right) \\\\ \\sigma\\left(z_2^{[1](i)}\\right)\\end{bmatrix}.\\tag{3}$$\n",
    "\n",
    "Затем выход скрытого слоя подается на выходной слой размера $n_y = 1$. Это было рассмотрено в предыдущем задании, единственные различия заключаются в том, что $a^{[1](i)}$ берется вместо $x^{(i)}$ и появляется обозначение слоя $^{[2]}$, чтобы идентифицировать все параметры и выходы:\n",
    "\n",
    "$$z^{[2](i)} = w_1^{[2]} a_1^{[1](i)} + w_2^{[2]} a_2^{[1](i)} + b^{[2]}= W^{[2]} a^{[1](i)} + b^{[2]},\\tag{4}$$\n",
    "\n",
    "&emsp; &emsp; $z^{[2](i)}$ и $b^{[2]}$ являются скалярами для этой модели, так как $\\left(n_y \\times 1\\right) = \\left(1 \\times 1\\right)$;\n",
    "\n",
    "&emsp; &emsp; $W^{[2]} = \\begin{bmatrix}w_1^{[2]} & w_2^{[2]}\\end{bmatrix}$ — вектор размера $\\left(n_y \\times n_h\\right) = \\left(1 \\times 2\\right)$.\n",
    "\n",
    "Наконец, та же сигмоидальная функция используется в качестве функции активации выходного слоя:\n",
    "\n",
    "$$a^{[2](i)} = \\sigma\\left(z^{[2](i)}\\right).\\tag{5}$$\n",
    "\n",
    "В математическом смысле модель нейронной сети с двумя слоями для каждого обучающего примера $x^{(i)}$ может быть записана с использованием выражений $(2) - (5)$. Давайте перепишем их рядом друг с другом для удобства:\n",
    "\n",
    "\\begin{align}\n",
    "z^{[1](i)} &= W^{[1]} x^{(i)} + b^{[1]},\\\\\n",
    "a^{[1](i)} &= \\sigma\\left(z^{[1](i)}\\right),\\\\\n",
    "z^{[2](i)} &= W^{[2]} a^{[1](i)} + b^{[2]},\\\\\n",
    "a^{[2](i)} &= \\sigma\\left(z^{[2](i)}\\right).\\\\\n",
    "\\tag{6}\n",
    "\\end{align}\n",
    "\n",
    "Обратите внимание, что все параметры, которые должны быть обучены в модели, записаны без индекса $^{(i)}$ — они независимы от входных данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, предсказания для некоторого примера $x^{(i)}$ могут быть сделаны, принимая выход $a^{[2](i)}$ и вычисляя $\\hat{y}$ следующим образом: $\\hat{y} = \\begin{cases} 1 & \\mbox{если } a^{[2](i)} > 0.5, \\\\ 0 & \\mbox{в противном случае }. \\end{cases}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "### 2.2 - Модель нейронной сети с двумя слоями для нескольких обучающих примеров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично модели с одним перцептроном, $m$ обучающих примеров могут быть занесены в матрице $X$ размера ($2 \\times m$), помещая $x^{(i)}$ в столбцы. Затем модель $(6)$ может быть переписана в терминах матричных умножений:\n",
    "\n",
    "\\begin{align}\n",
    "Z^{[1]} &= W^{[1]} X + b^{[1]},\\\\\n",
    "A^{[1]} &= \\sigma\\left(Z^{[1]}\\right),\\\\\n",
    "Z^{[2]} &= W^{[2]} A^{[1]} + b^{[2]},\\\\\n",
    "A^{[2]} &= \\sigma\\left(Z^{[2]}\\right),\\\\\n",
    "\\tag{7}\n",
    "\\end{align}\n",
    "\n",
    "где $b^{[1]}$ расширяется до матрицы размера $\\left(n_h \\times m\\right) = \\left(2 \\times m\\right)$, а $b^{[2]}$ — до вектора размера $\\left(n_y \\times m\\right) = \\left(1 \\times m\\right)$. Это хорошее упражнение для нас — посмотреть на выражения $(7)$ и проверить, что размеры матриц действительно соответствуют для выполнения необходимых умножений.\n",
    "\n",
    "Мы вывели выражения для выполнения прямого распространения. Время оценить нашу модель и обучить её."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.3'></a>\n",
    "### 2.3 - Функция потерь и обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оценки этой простой нейронной сети мы можем использовать ту же функцию потерь, что и в случае с одним перцептроном — логарифмическую функцию потерь. Изначально инициализированные веса были просто случайными значениями; теперь нам нужно обучить модель: найти такой набор параметров $W^{[1]}$, $b^{[1]}$, $W^{[2]}$, $b^{[2]}$, который минимизирует функцию потерь.\n",
    "\n",
    "Как и в предыдущем примере с нейронной сетью с одним перцептроном, функция потерь может быть записана как:\n",
    "\n",
    "$$\\mathcal{L}\\left(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}\\right) = \\frac{1}{m}\\sum_{i=1}^{m} L\\left(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}\\right) =  \\frac{1}{m}\\sum_{i=1}^{m}  \\large\\left(\\small - y^{(i)}\\log\\left(a^{[2](i)}\\right) - (1-y^{(i)})\\log\\left(1- a^{[2](i)}\\right)  \\large  \\right), \\small\\tag{8}$$\n",
    "\n",
    "где $y^{(i)} \\in \\{0,1\\}$ — это исходные метки, а $a^{[2](i)}$ — это непрерывные выходные значения шага прямого распространения (элементы массива $A^{[2]}$).\n",
    "\n",
    "Чтобы минимизировать это, мы можем использовать градиентный спуск, обновляя параметры по следующим выражениям:\n",
    "\n",
    "\\begin{align}\n",
    "W^{[1]} &= W^{[1]} - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]} },\\\\\n",
    "b^{[1]} &= b^{[1]} - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial b^{[1]} },\\\\\n",
    "W^{[2]} &= W^{[2]} - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial W^{[2]} },\\\\\n",
    "b^{[2]} &= b^{[2]} - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial b^{[2]} },\\\\\n",
    "\\tag{9}\n",
    "\\end{align}\n",
    "\n",
    "где $\\alpha$ — скорость обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выполнения обучения модели нам нужно теперь вычислить $\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}}$, $\\frac{\\partial \\mathcal{L} }{ \\partial b^{[1]}}$, $\\frac{\\partial \\mathcal{L} }{ \\partial W^{[2]}}$, $\\frac{\\partial \\mathcal{L} }{ \\partial b^{[2]}}$.\n",
    "\n",
    "Начнем с конца нейронной сети. Мы можем переписать здесь соответствующие выражения для $\\frac{\\partial \\mathcal{L} }{ \\partial W }$ и $\\frac{\\partial \\mathcal{L} }{ \\partial b }$ из нейронной сети с одним перцептроном:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial W } &= \n",
    "\\frac{1}{m}\\left(A-Y\\right)X^T,\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial b } &= \n",
    "\\frac{1}{m}\\left(A-Y\\right)\\mathbf{1},\\\\\n",
    "\\end{align}\n",
    "\n",
    "где $\\mathbf{1}$ — это просто ($m \\times 1$) вектор единиц. Наш один перцептрон теперь во втором слое, поэтому $W$ заменяется на $W^{[2]}$, $b$ на $b^{[2]}$, $A$ на $A^{[2]}$, а $X$ на $A^{[1]}$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial W^{[2]} } &= \n",
    "\\frac{1}{m}\\left(A^{[2]}-Y\\right)\\left(A^{[1]}\\right)^T,\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial b^{[2]} } &= \n",
    "\\frac{1}{m}\\left(A^{[2]}-Y\\right)\\mathbf{1}.\\\\\n",
    "\\tag{10}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте теперь найдем $\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial w_{1,1}^{[1]}} & \\frac{\\partial \\mathcal{L} }{ \\partial w_{2,1}^{[1]}} \\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial w_{1,2}^{[1]}} & \\frac{\\partial \\mathcal{L} }{ \\partial w_{2,2}^{[1]}} \\end{bmatrix}$. Согласно математическому выводу, $$\\frac{\\partial \\mathcal{L} }{ \\partial w_{1,1}^{[1]}}=\\frac{1}{m}\\sum_{i=1}^{m} \\left( \n",
    "\\left(a^{[2](i)} - y^{(i)}\\right) \n",
    "w_1^{[2]} \n",
    "\\left(a_1^{[1](i)}\\left(1-a_1^{[1](i)}\\right)\\right)\n",
    "x_1^{(i)}\\right)\\tag{11}$$\n",
    "\n",
    "Если сделаем это точно для каждого из элементов $\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}}$, то получим следующую матрицу:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}} = \\begin{bmatrix}\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial w_{1,1}^{[1]}} & \\frac{\\partial \\mathcal{L} }{ \\partial w_{2,1}^{[1]}} \\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial w_{1,2}^{[1]}} & \\frac{\\partial \\mathcal{L} }{ \\partial w_{2,2}^{[1]}} \\end{bmatrix}$$\n",
    "$$= \\frac{1}{m}\\begin{bmatrix}\n",
    "\\sum_{i=1}^{m} \\left( \\left(a^{[2](i)} - y^{(i)}\\right) w_1^{[2]} \\left(a_1^{[1](i)}\\left(1-a_1^{[1](i)}\\right)\\right)\n",
    "x_1^{(i)}\\right) & \n",
    "\\sum_{i=1}^{m} \\left( \\left(a^{[2](i)} - y^{(i)}\\right) w_1^{[2]} \\left(a_1^{[1](i)}\\left(1-a_1^{[1](i)}\\right)\\right)\n",
    "x_2^{(i)}\\right)  \\\\\n",
    "\\sum_{i=1}^{m} \\left( \\left(a^{[2](i)} - y^{(i)}\\right) w_2^{[2]} \\left(a_2^{[1](i)}\\left(1-a_2^{[1](i)}\\right)\\right)\n",
    "x_1^{(i)}\\right) & \n",
    "\\sum_{i=1}^{m} \\left( \\left(a^{[2](i)} - y^{(i)}\\right) w_2^{[2]} \\left(a_2^{[1](i)}\\left(1-a_2^{[1](i)}\\right)\\right)\n",
    "x_2^{(i)}\\right)\\end{bmatrix}\\tag{12}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотря на запись, можно заметить, что все составляющие и индексы довольно консистенты, так что всё это можно объединить в матричную форму. И это так. $\\left(W^{[2]}\\right)^T = \\begin{bmatrix}w_1^{[2]} \\\\ w_2^{[2]}\\end{bmatrix}$ размера $\\left(n_h \\times n_y\\right) = \\left(2 \\times 1\\right)$ может быть умножен на вектор $A^{[2]} - Y$ размера $\\left(n_y \\times m\\right) = \\left(1 \\times m\\right)$, в результате получив матрицу размера $\\left(n_h \\times m\\right) = \\left(2 \\times m\\right)$:\n",
    "\n",
    "$$\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)=\n",
    "\\begin{bmatrix}w_1^{[2]} \\\\ w_2^{[2]}\\end{bmatrix}\n",
    "\\begin{bmatrix}\\left(a^{[2](1)} - y^{(1)}\\right) &  \\cdots & \\left(a^{[2](m)} - y^{(m)}\\right)\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "\\left(a^{[2](1)} - y^{(1)}\\right) w_1^{[2]} & \\cdots & \\left(a^{[2](m)} - y^{(m)}\\right) w_1^{[2]} \\\\\n",
    "\\left(a^{[2](1)} - y^{(1)}\\right) w_2^{[2]} & \\cdots & \\left(a^{[2](m)} - y^{(m)}\\right) w_2^{[2]} \\end{bmatrix}$$.\n",
    "\n",
    "Теперь, взяв матрицу $A^{[1]}$ того же размера $\\left(n_h \\times m\\right) = \\left(2 \\times m\\right)$,\n",
    "\n",
    "$$A^{[1]}\n",
    "=\\begin{bmatrix}\n",
    "a_1^{[1](1)} & \\cdots & a_1^{[1](m)} \\\\\n",
    "a_2^{[1](1)} & \\cdots & a_2^{[1](m)} \\end{bmatrix},$$\n",
    "\n",
    "можно рассчитать:\n",
    "\n",
    "$$A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\n",
    "=\\begin{bmatrix}\n",
    "a_1^{[1](1)}\\left(1 - a_1^{[1](1)}\\right) & \\cdots & a_1^{[1](m)}\\left(1 - a_1^{[1](m)}\\right) \\\\\n",
    "a_2^{[1](1)}\\left(1 - a_2^{[1](1)}\\right) & \\cdots & a_2^{[1](m)}\\left(1 - a_2^{[1](m)}\\right) \\end{bmatrix},$$\n",
    "\n",
    "где \"$\\cdot$\" обозначает **поэлементное** умножение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью поэлементного умножения,\n",
    "\n",
    "$$\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)=\\begin{bmatrix}\n",
    "\\left(a^{[2](1)} - y^{(1)}\\right) w_1^{[2]}\\left(a_1^{[1](1)}\\left(1 - a_1^{[1](1)}\\right)\\right) & \\cdots & \\left(a^{[2](m)} - y^{(m)}\\right) w_1^{[2]}\\left(a_1^{[1](m)}\\left(1 - a_1^{[1](m)}\\right)\\right) \\\\\n",
    "\\left(a^{[2](1)} - y^{(1)}\\right) w_2^{[2]}\\left(a_2^{[1](1)}\\left(1 - a_2^{[1](1)}\\right)\\right) & \\cdots & \\left(a^{[2](m)} - y^{(m)}\\right) w_2^{[2]} \\left(a_2^{[1](m)}\\left(1 - a_2^{[1](m)}\\right)\\right) \\end{bmatrix}.$$\n",
    "\n",
    "Если мы выполним матричное умножение с $X^T$ размера $\\left(m \\times n_x\\right) = \\left(m \\times 2\\right)$, то получим матрицу размера $\\left(n_h \\times n_x\\right) = \\left(2 \\times 2\\right)$:\n",
    "\n",
    "$$\\left(\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)\\right)X^T = \n",
    "\\begin{bmatrix}\n",
    "\\left(a^{[2](1)} - y^{(1)}\\right) w_1^{[2]}\\left(a_1^{[1](1)}\\left(1 - a_1^{[1](1)}\\right)\\right) & \\cdots & \\left(a^{[2](m)} - y^{(m)}\\right) w_1^{[2]}\\left(a_1^{[1](m)}\\left(1 - a_1^{[1](m)}\\right)\\right) \\\\\n",
    "\\left(a^{[2](1)} - y^{(1)}\\right) w_2^{[2]}\\left(a_2^{[1](1)}\\left(1 - a_2^{[1](1)}\\right)\\right) & \\cdots & \\left(a^{[2](m)} - y^{(m)}\\right) w_2^{[2]} \\left(a_2^{[1](m)}\\left(1 - a_2^{[1](m)}\\right)\\right) \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1^{(1)} & x_2^{(1)} \\\\\n",
    "\\cdots & \\cdots \\\\\n",
    "x_1^{(m)} & x_2^{(m)}\n",
    "\\end{bmatrix}$$\n",
    "$$=\\begin{bmatrix}\n",
    "\\sum_{i=1}^{m} \\left( \\left(a^{[2](i)} - y^{(i)}\\right) w_1^{[2]} \\left(a_1^{[1](i)}\\left(1 - a_1^{[1](i)}\\right) \\right)\n",
    "x_1^{(i)}\\right) & \n",
    "\\sum_{i=1}^{m} \\left( \\left(a^{[2](i)} - y^{(i)}\\right) w_1^{[2]} \\left(a_1^{[1](i)}\\left(1-a_1^{[1](i)}\\right)\\right)\n",
    "x_2^{(i)}\\right)  \\\\\n",
    "\\sum_{i=1}^{m} \\left( \\left(a^{[2](i)} - y^{(i)}\\right) w_2^{[2]} \\left(a_2^{[1](i)}\\left(1-a_2^{[1](i)}\\right)\\right)\n",
    "x_1^{(i)}\\right) & \n",
    "\\sum_{i=1}^{m} \\left( \\left(a^{[2](i)} - y^{(i)}\\right) w_2^{[2]} \\left(a_2^{[1](i)}\\left(1-a_2^{[1](i)}\\right)\\right)\n",
    "x_2^{(i)}\\right)\\end{bmatrix}$$\n",
    "\n",
    "Это точно так же, как в выражении $(12)$! Таким образом, $\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}}$ можно записать как смесь умножений:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}} = \\frac{1}{m}\\left(\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)\\right)X^T\\tag{13},$$\n",
    "\n",
    "где \"$\\cdot$\" обозначает поэлементные умножения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вектор $\\frac{\\partial \\mathcal{L} }{ \\partial b^{[1]}}$ можно найти аналогичным образом, но последние составляющие в правиле цепочки будут равны $1$, т.е. $\\frac{\\partial z_1^{[1](i)}}{ \\partial b_1^{[1]}} = 1$. Таким образом,\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L} }{ \\partial b^{[1]}} = \\frac{1}{m}\\left(\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)\\right)\\mathbf{1},\\tag{14}$$\n",
    "\n",
    "где $\\mathbf{1}$ — это ($m \\times 1$) вектор единиц.\n",
    "\n",
    "Выражения $(10)$, $(13)$ и $(14)$ могут быть использованы для обновления параметров $(9)$ при выполнении обратного распространения:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial W^{[2]} } &= \n",
    "\\frac{1}{m}\\left(A^{[2]}-Y\\right)\\left(A^{[1]}\\right)^T,\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial b^{[2]} } &= \n",
    "\\frac{1}{m}\\left(A^{[2]}-Y\\right)\\mathbf{1},\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}} &= \\frac{1}{m}\\left(\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)\\right)X^T,\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial b^{[1]}} &= \\frac{1}{m}\\left(\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)\\right)\\mathbf{1},\\\\\n",
    "\\tag{15}\n",
    "\\end{align}\n",
    "\n",
    "где $\\mathbf{1}$ — это ($m \\times 1$) вектор единиц."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, чтобы понять глубоко и правильно, как работают нейронные сети и как они обучаются, **нам действительно нужно комбинированное знание линейной алгебры и математического анализа**. Но не волнуйтесь. В целом это не так страшно, если мы будем делать это шаг за шагом, точно понимая математику.\n",
    "\n",
    "Время реализовать все это в коде."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "### 2.2 - Набор данных\n",
    "\n",
    "Сначала давайте получим набор данных, с которым мы будем работать. Следующий код создаст $m=2000$ точек данных $(x_1, x_2)$ и сохранит их в массиве `NumPy` `X` размера $(2 \\times m)$ (в столбцах массива). Метки ($0$: синий, $1$: красный) будут сохранены в массиве `NumPy` `Y` размера $(1 \\times m)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "m = 2000\n",
    "samples, labels = make_blobs(n_samples=m, \n",
    "                             centers=([2.5, 3], [6.7, 7.9], [2.1, 7.9], [7.4, 2.8]), \n",
    "                             cluster_std=1.1,\n",
    "                             random_state=0)\n",
    "labels[(labels == 0) | (labels == 1)] = 1\n",
    "labels[(labels == 2) | (labels == 3)] = 0\n",
    "X = np.transpose(samples)\n",
    "Y = labels.reshape((1, m))\n",
    "\n",
    "plt.scatter(X[0, :], X[1, :], c=Y, cmap=colors.ListedColormap(['blue', 'red']));\n",
    "\n",
    "print('Форма X: ' + str(X.shape))\n",
    "print('Форма Y: ' + str(Y.shape))\n",
    "print('У нас есть m = %d обучающих примеров!' % (m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.3'></a>\n",
    "### 2.3 - Определение функции активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex01'></a>\n",
    "### Упражнение 1\n",
    "\n",
    "Определите сигмоидальную функцию активации $\\sigma\\left(z\\right) =\\frac{1}{1+e^{-z}} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    ### START CODE HERE ### (~ 1 line of code)\n",
    "    res = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"sigmoid(-2) = \" + str(sigmoid(-2)))\n",
    "print(\"sigmoid(0) = \" + str(sigmoid(0)))\n",
    "print(\"sigmoid(3.5) = \" + str(sigmoid(3.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Ожидаемый вывод__\n",
    "\n",
    "Примечание: значения могут отличаться в последних десятичных знаках.\n",
    "\n",
    "```Python\n",
    "sigmoid(-2) = 0.11920292202211755\n",
    "sigmoid(0) = 0.5\n",
    "sigmoid(3.5) = 0.9706877692486436\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w3_unittest.test_sigmoid(sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Реализация модели нейронной сети с двумя слоями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.1'></a>\n",
    "### 3.1 - Определение структуры нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex02'></a>\n",
    "### Упражнение 2\n",
    "\n",
    "Определите три переменные:\n",
    "- `n_x`: размер входного слоя\n",
    "- `n_h`: размер скрытого слоя (пока установите его равным 2)\n",
    "- `n_y`: размер выходного слоя"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Совет</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    Используйте размеры X и Y, чтобы найти n_x и n_y:\n",
    "    <li>размер входного слоя n_x равен размеру входных векторов, размещенных в столбцах массива X,</li>\n",
    "    <li>выходы для каждого из обучающих примеров будут храниться в столбцах массива Y.</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: layer_sizes\n",
    "\n",
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- входной набор данных формы (размер входа, число примеров)\n",
    "    Y -- метки формы (размер выхода, число примеров)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- размер входного слоя\n",
    "    n_h -- размер скрытого слоя\n",
    "    n_y -- размер выходного слоя\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (~ 3 lines of code)\n",
    "    # Размер входного слоя.\n",
    "    n_x = None\n",
    "    # Размер скрытого слоя.\n",
    "    n_h = None\n",
    "    # Размер выходного слоя.\n",
    "    n_y = None\n",
    "    ### END CODE HERE ###\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "(n_x, n_h, n_y) = layer_sizes(X, Y)\n",
    "print(\"Размер входного слоя: n_x = \" + str(n_x))\n",
    "print(\"Размер скрытого слоя: n_h = \" + str(n_h))\n",
    "print(\"Размер выходного слоя: n_y = \" + str(n_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Ожидаемый вывод__\n",
    "\n",
    "```Python\n",
    "Размер входного слоя: n_x = 2\n",
    "Размер скрытого слоя: n_h = 2\n",
    "Размер выходного слоя: n_y = 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "w3_unittest.test_layer_sizes(layer_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.2'></a>\n",
    "### 3.2 - Инициализация параметров модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex03'></a>\n",
    "### Упражнение 3\n",
    "\n",
    "Реализуйте функцию `initialize_parameters()`.\n",
    "\n",
    "**Инструкции**:\n",
    "- Убедитесь, что размеры ваших параметров правильные. При необходимости обратитесь к рисунку нейронной сети выше.\n",
    "- Вы будете инициализировать матрицу весов случайными значениями.\n",
    "    - Используйте: `np.random.randn(a,b) * 0.01` для инициализации матрицы размера (a,b).\n",
    "- Вы будете инициализировать вектор смещения нулями.\n",
    "    - Используйте: `np.zeros((a,b))` для инициализации матрицы размера (a,b) нулями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Аргумент:\n",
    "    n_x -- размер входного слоя\n",
    "    n_h -- размер скрытого слоя\n",
    "    n_y -- размер выходного слоя\n",
    "    \n",
    "    Returns:\n",
    "    params -- словарь Python, содержащий ваши параметры:\n",
    "                    W1 -- матрица весов размера (n_h, n_x)\n",
    "                    b1 -- вектор смещения размера (n_h, 1)\n",
    "                    W2 -- матрица весов размера (n_y, n_h)\n",
    "                    b2 -- вектор смещения размера (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (~ 4 lines of code)\n",
    "    W1 = None\n",
    "    b1 = None\n",
    "    W2 = None\n",
    "    b2 = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Ожидаемый вывод__ \n",
    "Примечание: элементы массивов W1 и W2 могут отличаться из-за случайной инициализации. Вы можете попробовать перезапустить ядро, чтобы получить одинаковые значения.\n",
    "\n",
    "```Python\n",
    "W1 = [[ 0.01788628  0.0043651 ]\n",
    " [ 0.00096497 -0.01863493]]\n",
    "b1 = [[0.]\n",
    " [0.]]\n",
    "W2 = [[-0.00277388 -0.00354759]]\n",
    "b2 = [[0.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Примечание: \n",
    "# Фактические значения здесь не проверяются в юнит-тестах (из-за случайной инициализации).\n",
    "w3_unittest.test_initialize_parameters(initialize_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.3'></a>\n",
    "### 3.3 - Цикл"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex04'></a>\n",
    "### Упражнение 4\n",
    "\n",
    "Реализуйте `forward_propagation()`.\n",
    "\n",
    "**Инструкции**:\n",
    "- Посмотрите выше на математическое представление $(7)$ вашего классификатора (раздел [2.2](#2.2)):\n",
    "\\begin{align}\n",
    "Z^{[1]} &= W^{[1]} X + b^{[1]},\\\\\n",
    "A^{[1]} &= \\sigma\\left(Z^{[1]}\\right),\\\\\n",
    "Z^{[2]} &= W^{[2]} A^{[1]} + b^{[2]},\\\\\n",
    "A^{[2]} &= \\sigma\\left(Z^{[2]}\\right).\\\\\n",
    "\\end{align}\n",
    "- Шаги, которые нужно реализовать:\n",
    "    1. Извлеките каждый параметр из словаря \"parameters\" (который является выходом `initialize_parameters()`) с использованием `parameters[\"..\"]`.\n",
    "    2. Реализуйте прямое распространение. Вычислите `Z1`, умножив матрицы `W1`, `X` и добавив вектор `b1`. Затем найдите `A1` с использованием функции активации `sigmoid`. Выполните аналогичные вычисления для `Z2` и `A2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Аргумент:\n",
    "    X -- входные данные размера (n_x, m)\n",
    "    parameters -- словарь Python, содержащий ваши параметры (выход функции инициализации)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- сигмоидальный выход второго активационного слоя\n",
    "    cache -- словарь Python, содержащий Z1, A1, Z2, A2 \n",
    "    (это упрощает вычисления на этапе обратного распространения)\n",
    "    \"\"\"\n",
    "    # Извлечение каждого параметра из словаря \"parameters\".\n",
    "    ### START CODE HERE ### (~ 4 lines of code)\n",
    "    W1 = None\n",
    "    b1 = None\n",
    "    W2 = None\n",
    "    b2 = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Реализуем прямое распространение для вычисления A2.\n",
    "    ### START CODE HERE ### (~ 4 lines of code)\n",
    "    Z1 = None\n",
    "    A1 = None\n",
    "    Z2 = None\n",
    "    A2 = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(A2.shape == (n_y, X.shape[1]))\n",
    "\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "A2, cache = forward_propagation(X, parameters)\n",
    "\n",
    "print(A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Ожидаемый вывод__ \n",
    "Примечание: элементы массива A2 могут отличаться в зависимости от начальных параметров. Если вы хотите получить точно такой же вывод, попробуйте перезапустить ядро и снова выполнить блокнот.\n",
    "\n",
    "```Python\n",
    "[[0.49920157 0.49922234 0.49921223 ... 0.49921215 0.49921043 0.49920665]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Примечание: \n",
    "# Фактические значения не проверяются здесь в юнит-тестах (из-за случайной инициализации).\n",
    "w3_unittest.test_forward_propagation(forward_propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помните, что ваши веса были только инициализированы некоторыми случайными значениями, поэтому модель ещё не обучена."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex05'></a>\n",
    "### Упражнение 5\n",
    "\n",
    "Определите функцию потерь $(8)$, которая будет использоваться для обучения модели:\n",
    "\n",
    "$$\\mathcal{L}\\left(W, b\\right)  = \\frac{1}{m}\\sum_{i=1}^{m}  \\large\\left(\\small - y^{(i)}\\log\\left(a^{(i)}\\right) - (1-y^{(i)})\\log\\left(1- a^{(i)}\\right)  \\large  \\right) \\small.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Вычисляет логарифмическую функцию потерь\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- Выход нейронной сети размером (1, количество примеров)\n",
    "    Y -- Вектор \"истинных\" меток размером (1, количество примеров)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- логарифмическая функция потерь\n",
    "    \n",
    "    \"\"\"\n",
    "    # Количество примеров.\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    ### START CODE HERE ### (~ 2 lines of code)\n",
    "    logloss = None\n",
    "    cost = None\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"cost = \" + str(compute_cost(A2, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Ожидаемый результат__ \n",
    "Примечание: элементы массивов W1 и W2 могут отличаться!\n",
    "\n",
    "```Python\n",
    "cost = 0.6931477703826823\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Примечание: \n",
    "# Фактические значения не проверяются здесь в юнит-тестах (из-за случайной инициализации).\n",
    "w3_unittest.test_compute_cost(compute_cost, A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислите частные производные, как показано в $(15)$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial W^{[2]} } &= \n",
    "\\frac{1}{m}\\left(A^{[2]}-Y\\right)\\left(A^{[1]}\\right)^T,\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial b^{[2]} } &= \n",
    "\\frac{1}{m}\\left(A^{[2]}-Y\\right)\\mathbf{1},\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}} &= \\frac{1}{m}\\left(\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)\\right)X^T,\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial b^{[1]}} &= \\frac{1}{m}\\left(\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)\\right)\\mathbf{1}.\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Реализует обратное распространение, вычисляя градиенты\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- словарь python, содержащий наши параметры \n",
    "    cache -- словарь python, содержащий Z1, A1, Z2, A2\n",
    "    X -- входные данные размером (n_x, количество примеров)\n",
    "    Y -- вектор \"истинных\" меток размером (n_y, количество примеров)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- словарь python, содержащий градиенты по различным параметрам\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Сначала извлекаем W из словаря \"parameters\".\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "    # Также извлекаем A1 и A2 из словаря \"cache\".\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    # Обратное распространение: вычисляем частные производные, обозначенные как dW1, db1, dW2, db2 для упрощения. \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = 1/m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2, axis = 1, keepdims = True)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * A1 * (1 - A1)\n",
    "    dW1 = 1/m * np.dot(dZ1, X.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis = 1, keepdims = True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads\n",
    "\n",
    "grads = backward_propagation(parameters, cache, X, Y)\n",
    "\n",
    "print(\"dW1 = \" + str(grads[\"dW1\"]))\n",
    "print(\"db1 = \" + str(grads[\"db1\"]))\n",
    "print(\"dW2 = \" + str(grads[\"dW2\"]))\n",
    "print(\"db2 = \" + str(grads[\"db2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex06'></a>\n",
    "### Упражнение 6\n",
    "\n",
    "Реализуйте `update_parameters()`.\n",
    "\n",
    "**Инструкции**:\n",
    "- Обновите параметры, как показано в $(9)$ (раздел [2.3](#2.3)):\n",
    "\\begin{align}\n",
    "W^{[1]} &= W^{[1]} - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]} },\\\\\n",
    "b^{[1]} &= b^{[1]} - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial b^{[1]} },\\\\\n",
    "W^{[2]} &= W^{[2]} - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial W^{[2]} },\\\\\n",
    "b^{[2]} &= b^{[2]} - \\alpha \\frac{\\partial \\mathcal{L} }{ \\partial b^{[2]} }.\\\\\n",
    "\\end{align}\n",
    "- Шаги, которые необходимо реализовать:\n",
    "    1. Извлеките каждый параметр из словаря \"parameters\" (который является выходом `initialize_parameters()`) с помощью `parameters[\"..\"]`.\n",
    "    2. Извлеките каждую производную из словаря \"grads\" (который является выходом `backward_propagation()`) с помощью `grads[\"..\"]`.\n",
    "    3. Обновите параметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate=1.2):\n",
    "    \"\"\"\n",
    "    Обновляет параметры, используя правило обновления градиентного спуска\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- словарь python, содержащий параметры \n",
    "    grads -- словарь python, содержащий градиенты\n",
    "    learning_rate -- скорость обучения для градиентного спуска\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- словарь python, содержащий обновленные параметры \n",
    "    \"\"\"\n",
    "    # Извлекаем каждый параметр из словаря \"parameters\".\n",
    "    ### START CODE HERE ### (~ 4 lines of code)\n",
    "    W1 = None\n",
    "    b1 = None\n",
    "    W2 = None\n",
    "    b2 = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Извлекаем каждый градиент из словаря \"grads\".\n",
    "    ### START CODE HERE ### (~ 4 lines of code)\n",
    "    dW1 = None\n",
    "    db1 = None\n",
    "    dW2 = None\n",
    "    db2 = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Правило обновления для каждого параметра.\n",
    "    ### START CODE HERE ### (~ 4 lines of codeа)\n",
    "    W1 = None\n",
    "    b1 = None\n",
    "    W2 = None\n",
    "    b2 = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_updated = update_parameters(parameters, grads)\n",
    "\n",
    "print(\"W1 updated = \" + str(parameters_updated[\"W1\"]))\n",
    "print(\"b1 updated = \" + str(parameters_updated[\"b1\"]))\n",
    "print(\"W2 updated = \" + str(parameters_updated[\"W2\"]))\n",
    "print(\"b2 updated = \" + str(parameters_updated[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Ожидаемый результат__ \n",
    "Примечание: фактические значения могут отличаться!\n",
    "\n",
    "```Python\n",
    "W1 updated = [[ 0.01790427  0.00434496]\n",
    " [ 0.00099046 -0.01866419]]\n",
    "b1 updated = [[-6.13449205e-07]\n",
    " [-8.47483463e-07]]\n",
    "W2 updated = [[-0.00238219 -0.00323487]]\n",
    "b2 updated = [[0.00094478]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w3_unittest.test_update_parameters(update_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.4'></a>\n",
    "### 3.4 - Интегрируйте части 3.1, 3.2 и 3.3 в nn_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex07'></a>\n",
    "### Упражнение 7\n",
    "\n",
    "Создайте модель нейронной сети в `nn_model()`.\n",
    "\n",
    "**Инструкции**: Модель нейронной сети должна использовать предыдущие функции в правильном порядке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: nn_model\n",
    "\n",
    "def nn_model(X, Y, n_h, num_iterations=10, learning_rate=1.2, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- набор данных размером (n_x, количество примеров)\n",
    "    Y -- метки размером (n_y, количество примеров)\n",
    "    num_iterations -- количество итераций в цикле\n",
    "    learning_rate -- параметр скорости обучения для градиентного спуска\n",
    "    print_cost -- если True, выводим функцию потерь на каждой итерации\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- параметры обученной модели. Затем их можно использовать для предсказания.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    # Инициализация параметров.\n",
    "    ### START CODE HERE ### (~ 1 line of code)\n",
    "    parameters = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Цикл.\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        ### START CODE HERE ### (~ 4 lines of code)\n",
    "        # Прямое распространение. Входы: \"X, параметры\". Выходы: \"A2, cache\".\n",
    "        A2, cache = None\n",
    "        \n",
    "        # Функция потерь. Входы: \"A2, Y\". Выходы: \"cost\".\n",
    "        cost = None\n",
    "        \n",
    "        # Обратное распространение. Входы: \"parameters, cache, X, Y\". Выходы: \"grads\".\n",
    "        grads = None\n",
    "        \n",
    "        # Обновление параметров градиентного спуска. Входы: \"parameters, grads, learning_rate\". Выходы: \"parameters\".\n",
    "        parameters = None\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Печатаем функцию потерь на каждой итерации.\n",
    "        if print_cost:\n",
    "            print (\"Функция потерь после итерации %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = nn_model(X, Y, n_h=2, num_iterations=3000, learning_rate=1.2, print_cost=True)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "\n",
    "W1 = parameters[\"W1\"]\n",
    "b1 = parameters[\"b1\"]\n",
    "W2 = parameters[\"W2\"]\n",
    "b2 = parameters[\"b2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Ожидаемый результат__ \n",
    "Примечание: фактические значения могут отличаться!\n",
    "\n",
    "```Python\n",
    "Функция потерь после итерации 0: 0.693148\n",
    "Функция потерь после итерации 1: 0.693147\n",
    "Функция потерь после итерации 2: 0.693147\n",
    "Функция потерь после итерации 3: 0.693147\n",
    "Функция потерь после итерации 4: 0.693147\n",
    "Функция потерь после итерации 5: 0.693147\n",
    "...\n",
    "Функция потерь после итерации 2995: 0.209524\n",
    "Функция потерь после итерации 2996: 0.208025\n",
    "Функция потерь после итерации 2997: 0.210427\n",
    "Функция потерь после итерации 2998: 0.208929\n",
    "Функция потерь после итерации 2999: 0.211306\n",
    "W1 = [[ 2.14274251 -1.93155541]\n",
    " [ 2.20268789 -2.1131799 ]]\n",
    "b1 = [[-4.83079243]\n",
    " [ 6.2845223 ]]\n",
    "W2 = [[-7.21370685  7.0898022 ]]\n",
    "b2 = [[-3.48755239]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Примечание: \n",
    "# Фактические значения не проверяются здесь в юнит-тестах (из-за случайной инициализации).\n",
    "w3_unittest.test_nn_model(nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Финальные параметры модели могут быть использованы для нахождения границы разделения классов и для предсказаний. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex08'></a>\n",
    "### Упражнение 8\n",
    "\n",
    "Вычислите вероятности, используя прямое распространение, и сделайте классификацию на 0/1, используя 0.5 как порог."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(X, parameters):\n",
    "    \"\"\"\n",
    "    Используя изученные параметры, предсказывает класс для каждого примера в X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- словарь python, содержащий ваши параметры \n",
    "    X -- входные данные размером (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- вектор предсказаний нашей модели (синий: 0 / красный: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (~ 2 lines of code)\n",
    "    A2, cache = None\n",
    "    predictions = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = np.array([[2, 8, 2, 8], [2, 8, 8, 2]])\n",
    "Y_pred = predict(X_pred, parameters)\n",
    "\n",
    "print(f\"Координаты (в столбцах):\\n{X_pred}\")\n",
    "print(f\"Предсказания:\\n{Y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Ожидаемый результат__ \n",
    "\n",
    "```Python\n",
    "Координаты (в столбцах):\n",
    "[[2 8 2 8]\n",
    " [2 8 8 2]]\n",
    "Предсказания:\n",
    "[[ True  True False False]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w3_unittest.test_predict(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте визуализируем границу. Не беспокойтесь, если не понимаете функцию `plot_decision_boundary` построчно - она просто делает предсказание для некоторых точек на плоскости и отображает их в виде контурного графика (просто два цвета - синий и красный)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(predict, parameters, X, Y):\n",
    "    # Определение границ области.\n",
    "    min1, max1 = X[0, :].min()-1, X[0, :].max()+1\n",
    "    min2, max2 = X[1, :].min()-1, X[1, :].max()+1\n",
    "    # Определение масштаба x и y.\n",
    "    x1grid = np.arange(min1, max1, 0.1)\n",
    "    x2grid = np.arange(min2, max2, 0.1)\n",
    "    # Создание всех линий и строк сетки.\n",
    "    xx, yy = np.meshgrid(x1grid, x2grid)\n",
    "    # Преобразование каждой сетки в вектор.\n",
    "    r1, r2 = xx.flatten(), yy.flatten()\n",
    "    r1, r2 = r1.reshape((1, len(r1))), r2.reshape((1, len(r2)))\n",
    "    # Вертикальная укладка векторов, чтобы создать x1,x2 вход для модели.\n",
    "    grid = np.vstack((r1,r2))\n",
    "    # Делает предсказания для сетки.\n",
    "    predictions = predict(grid, parameters)\n",
    "    # Изменение размера предсказаний обратно в сетку.\n",
    "    zz = predictions.reshape(xx.shape)\n",
    "    # Построение сетки значений x, y и z как поверхности.\n",
    "    plt.contourf(xx, yy, zz, cmap=plt.cm.Spectral.reversed())\n",
    "    plt.scatter(X[0, :], X[1, :], c=Y, cmap=colors.ListedColormap(['blue', 'red']));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построение границы решения.\n",
    "plot_decision_boundary(predict, parameters, X, Y)\n",
    "plt.title(\"Граница принятия решения для размера скрытого слоя \" + str(n_h))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, более сложные задачи классификации могут быть решены с помощью нейронной сети с двумя слоями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Другой набор данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создим немного другой набор данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "samples, labels = make_blobs(n_samples=n_samples, \n",
    "                             centers=([2.5, 3], [6.7, 7.9], [2.1, 7.9], [7.4, 2.8]), \n",
    "                             cluster_std=1.1,\n",
    "                             random_state=0)\n",
    "labels[(labels == 0)] = 0\n",
    "labels[(labels == 1)] = 1\n",
    "labels[(labels == 2) | (labels == 3)] = 1\n",
    "X_2 = np.transpose(samples)\n",
    "Y_2 = labels.reshape((1,n_samples))\n",
    "\n",
    "plt.scatter(X_2[0, :], X_2[1, :], c=Y_2, cmap=colors.ListedColormap(['blue', 'red']));\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что при построении вашей нейронной сети, количество узлов в скрытом слое может быть задано как параметр. Попробуйте изменить этот параметр и исследовать результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters_2 = nn_model(X_2, Y_2, n_h=1, num_iterations=3000, learning_rate=1.2, print_cost=False)\n",
    "parameters_2 = nn_model(X_2, Y_2, n_h=2, num_iterations=3000, learning_rate=1.2, print_cost=False)\n",
    "# parameters_2 = nn_model(X_2, Y_2, n_h=15, num_iterations=3000, learning_rate=1.2, print_cost=False)\n",
    "\n",
    "# Эта функция вызовет функцию predict \n",
    "plot_decision_boundary(predict, parameters_2, X_2, Y_2)\n",
    "plt.title(\"Граница принятия решения\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что есть несколько неправильно классифицированных точек - реальные наборы данных обычно линейно неразделимы, и будет небольшой процент ошибок. Более того, мы не хотим строить модель, которая слишком точно соответствует конкретному набору данных - она может не предсказать будущие наблюдения. Эта проблема известна как **переобучение**."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C1_W1_Assignment_Solution.ipynb",
   "provenance": []
  },
  "coursera": {
   "schema_names": [
    "AI4MC1-1"
   ]
  },
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Содержание",
   "title_sidebar": "Содержимое",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
