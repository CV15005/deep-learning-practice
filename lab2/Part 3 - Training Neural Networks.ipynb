{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение нейронных сетей\n",
    "\n",
    "Сеть, которую мы построили в предыдущей части, пока не обучена. Нейронные сети с нелинейными активациями работают как универсальные аппроксиматоры функций. Существует некоторый функционал, который отображает ваш ввод на выход. Например, изображения рукописных цифр на вероятности классов. Сила нейронных сетей заключается в том, что мы можем их обучить для аппроксимации этой функции, а в принципе любой функции, если у нас достаточно данных и вычислительных ресурсов.\n",
    "\n",
    "<img src=\"assets/function_approx.png\" width=500px>\n",
    "\n",
    "Сначала сеть является наивной, она не знает функцию, которая отображает входные данные на выходные. Мы обучаем сеть, показывая ей примеры реальных данных, а затем корректируя параметры сети, чтобы она приближала эту функцию.\n",
    "\n",
    "Чтобы найти эти параметры, нам нужно знать, насколько плохо сеть предсказывает реальные выходы. Для этого мы вычисляем **функцию потерь** (loss function, cost), меру нашей ошибки предсказания. Например, среднеквадратическая функция потерь часто используется в задачах регрессии и бинарной классификации.\n",
    "\n",
    "$$\n",
    "\\large \\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "\n",
    "где $n$ — количество обучающих примеров, $y_i$ — истинные метки, а $\\hat{y}_i$ — предсказанные метки.\n",
    "\n",
    "Минимизируя эту потерю относительно параметров сети, мы можем найти конфигурации, где потери находятся на минимуме, и сеть может предсказывать правильные метки с высокой точностью. Мы находим этот минимум с помощью процесса, называемого **градиентным спуском**. Градиент — это наклон (slope) функции потерь и указывает в направлении самого быстрого изменения. Чтобы добраться до минимума за наименьшее время, мы хотим следовать градиенту (вниз). Вы можете представить это как спуск с горы, следуя по самым крутым склонам к подножию.\n",
    "\n",
    "<img src='assets/gradient_descent.png' width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обратное распространение\n",
    "\n",
    "Для сетей с одним слоем градиентный спуск легко реализовать. Однако для более глубоких многослойных нейронных сетей, таких как та, что мы построили, это более сложно. Достаточно сложно, что прошло около 30 лет, прежде чем исследователи поняли, как обучать многослойные сети.\n",
    "\n",
    "Обучение многослойных сетей осуществляется с помощью **обратного распространения**, что на самом деле является лишь применением правила цепочки дифференцироввания сложной функции из матанализа. Легче всего понимать это, если мы преобразуем сеть из двух слоев в графическое представление.\n",
    "\n",
    "<img src='assets/backprop_diagram.png' width=550px>\n",
    "\n",
    "В прямом проходе через сеть наши данные и операции проходят снизу вверх. Мы пропускаем вход $x$ через линейное преобразование $L_1$ с весами $W_1$ и смещениями $b_1$. Выход затем проходит через сигмоиду $S$ и ещё одно линейное преобразование $L_2$. Наконец, мы вычисляем потерю $\\ell$. Мы используем функцию потерь в качестве меры того, насколько плохи предсказания сети. Цель затем — настроить веса и смещения, чтобы минимизировать функцию потерь.\n",
    "\n",
    "Чтобы обучить веса с помощью градиентного спуска, мы пропускаем градиент потерь назад через сеть. Каждая операция имеет некоторый градиент между входами и выходами. Отправляя градиенты назад, мы умножаем входящий градиент на градиент операции. Математически это на самом деле просто вычисление градиента потерь относительно весов с использованием правила цепочки.\n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial S}{\\partial L_1} \\frac{\\partial L_2}{\\partial S} \\frac{\\partial \\ell}{\\partial L_2}\n",
    "$$\n",
    "\n",
    "**Примечание:** Здесь опускаются некоторые детали из векторного исчисления, но они не обязательны для общего понимания происходящего.\n",
    "\n",
    "Мы обновляем наши веса, используя этот градиент с некоторой скоростью обучения $\\alpha$.\n",
    "\n",
    "$$\n",
    "\\large W^\\prime_1 = W_1 - \\alpha \\frac{\\partial \\ell}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "Скорость обучения $\\alpha$ выбирается таким образом, чтобы шаги обновления весов были достаточно малыми, чтобы итеративный метод сошелся на минимуме."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции потерь в PyTorch\n",
    "\n",
    "Давайте начнем с того, как мы вычисляем функцию потерь с помощью PyTorch. Через модуль `nn` PyTorch предоставляет функции потерь, например, кросс-энтропию (`nn.CrossEntropyLoss`). Обычно вы увидите, что потеря присваивается переменной `criterion`. Как было отмечено в последней части, при решении задач классификации, таких как в примере с MNIST, мы используем функцию softmax для предсказания вероятностей классов. С выводом softmax вы хотите использовать кросс-энтропию в качестве функции потерь. Чтобы действительно вычислить функцию потерь, вы сначала определяете критерий, а затем передаете выход вашей сети и правильные метки.\n",
    "\n",
    "Что-то очень важное, о чем необходимо упомянуть. Обратимся к [документации для `nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss),\n",
    "\n",
    "> This criterion combines `nn.LogSoftmax()` and `nn.NLLLoss()` in one single class.\n",
    ">\n",
    "> The input is expected to contain scores for each class.\n",
    "\n",
    "Это значит, что нам нужно передать сырой выход нашей сети в функцию потерь, а не выход функции softmax. Этот сырой выход обычно называется *логитами* (logits) или *оценками* (scores). Мы используем логиты, поскольку softmax дает вам вероятности, которые часто бывают очень близкими к нулю или единице, но числа с плавающей точкой не могут точно представить значения, близкие к нулю или единице ([узнать больше здесь](https://docs.python.org/3/tutorial/floatingpoint.html)). Обычно лучше избегать вычислений с вероятностями, часто мы используем логарифмы вероятностей (log-probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Наборы данных MNIST размещены на yann.lecun.com, который переместился под защиту CloudFlare\n",
    "# Запустите этот скрипт, чтобы сделать возможной загрузку этого набора данных\n",
    "# Ссылка: https://github.com/pytorch/vision/issues/1938\n",
    "\n",
    "from six.moves import urllib\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Определим трансформацию для нормализации данных\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# Загрузим обучающие данные\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим сеть прямого распространения\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10))\n",
    "\n",
    "# Определим функцию потерь\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Получим наши данные\n",
    "images, labels = next(iter(trainloader))\n",
    "# Разворачиваем изображения\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Прямой проход, получаем наши логиты\n",
    "logits = model(images)\n",
    "# Вычисляем функцию потерь, передавая логиты и истинные метки\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С другой стороны, удобнее построить модель с логарифмическим softmax выводом, используя `nn.LogSoftmax` или `F.log_softmax` ([документация](https://pytorch.org/docs/stable/nn.html#torch.nn.LogSoftmax)). Затем вы можете получить фактические вероятности, взяв экспоненту `torch.exp(output)`. С выводом логарифмического softmax вам необходимо использовать обратный логарифм правдоподобия (negative log likelihood loss), `nn.NLLLoss` ([документация](https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss)).\n",
    "\n",
    ">**Упражнение:** Постройте модель, которая возвращает логарифмический softmax в качестве вывода и вычисляет функцию потерь, используя обратный логарифм правдоподобия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "\n",
    "# Создайте сеть прямого распространения\n",
    "model = \n",
    "\n",
    "# Определите функцию потерь\n",
    "criterion = \n",
    "\n",
    "# Получите наши данные\n",
    "images, labels = next(iter(trainloader))\n",
    "# Разверните изображения в вектор-строки\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Прямой проход, получите логарифмические вероятности\n",
    "logps = model(images)\n",
    "# Вычислите функцию потерь, используя логарифмические вероятности и метки\n",
    "loss = criterion(logps, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "Теперь, когда мы знаем, как вычислить функцию потерь, как мы можем использовать это для выполнения обратного распространения? Torch предоставляет модуль `autograd`, который автоматически вычисляет градиенты тензоров. Мы можем использовать его для вычисления градиентов всех наших параметров относительно функции потерь. Autograd работает, отслеживая операции, выполняемые над тензорами, а затем проходя обратно через эти операции, вычисляя градиенты по пути. Чтобы убедиться, что PyTorch отслеживает операции над тензором и вычисляет градиенты, вам нужно установить `requires_grad = True` для тензора. Вы можете сделать это при создании с помощью ключевого слова `requires_grad` или в любое время с помощью `x.requires_grad_(True)`.\n",
    "\n",
    "Вы можете отключить градиенты для блока кода с помощью `torch.no_grad()`:\n",
    "```python\n",
    "x = torch.zeros(1, requires_grad=True)\n",
    ">>> with torch.no_grad():\n",
    "...     y = x * 2\n",
    ">>> y.requires_grad\n",
    "False\n",
    "\n",
    "Кроме того, вы можете включить или отключить градиенты полностью с помощью `torch.set_grad_enabled(True|False)`.\n",
    "\n",
    "Градиенты вычисляются относительно некоторой переменной `z` с помощью `z.backward()`. Это выполняет обратный проход по операциям, которые создали `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x**2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже мы можем увидеть операцию, которая создала `y`, операцию возведения в степень `PowBackward0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## grad_fn показывает функцию, которая сгенерировала эту переменную\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модуль autograd отслеживает эти операции и знает, как вычислить градиент для каждой из них. Таким образом, он способен вычислить градиенты для цепочки операций относительно любого одного тензора. Давайте уменьшим тензор `y` до скалярного значения, среднего.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы можете проверить градиенты для `x` и `y`, но в данный момент они пустые."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы вычислить градиенты, вам нужно запустить метод `.backward` на переменной, например `z`. Это вычислит градиент для `z` относительно `x`\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "print(x/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эти вычисления градиентов особенно полезны для нейронных сетей. Для обучения нам нужны градиенты весов относительно функции потерь. С помощью PyTorch мы запускаем данные вперед через сеть, чтобы вычислить потерю, а затем идем назад, чтобы вычислить градиенты относительно функции потерь. Как только у нас есть градиенты, мы можем сделать шаг градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss и Autograd вместе\n",
    "\n",
    "Когда мы создаем сеть с помощью PyTorch, все параметры инициализируются с `requires_grad = True`. Это означает, что когда мы вычисляем функцию потерь и вызываем `loss.backward()`, вычисляются градиенты для параметров. Эти градиенты используются для обновления весов с помощью градиентного спуска. Ниже вы можете увидеть пример вычисления градиентов с помощью обратного распространения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим сеть прямого распространения\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model(images)\n",
    "loss = criterion(logps, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Перед обратным распространением: \\n', model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('После обратного распространения: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение сети\n",
    "\n",
    "Есть еще одна последняя деталь, которую нам нужно установить для обучения — оптимизатор, который мы будем использовать для обновления весов с помощью градиентов. Мы получаем их из [Pytorch пакета `optim`](https://pytorch.org/docs/stable/optim.html). Например, мы можем использовать стохастический градиентный спуск — `optim.SGD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Оптимизаторы принимают параметры, которые будут обновляться, и скорость обучения\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы знаем, как использовать отдельные части, так что пришло время увидеть, как они работают вместе. Рассмотрим всего один шаг обучения перед тем, как сделать перебор всех данных. Этапы следующие:\n",
    "\n",
    "* Выполним прямой проход через сеть \n",
    "* Используем выход сети для вычисления функции потерь\n",
    "* Выполним обратный проход через сеть с помощью `loss.backward()`, чтобы вычислить градиенты\n",
    "* Сделаем шаг с помощью оптимизатора для обновления весов\n",
    "\n",
    "Ниже выполним один шаг обучения и выведем веса и градиенты, чтобы увидеть, как они меняются. Обратите внимание, что есть строка кода `optimizer.zero_grad()`. Когда вы выполняете несколько обратных проходов с одними и теми же параметрами, градиенты накапливаются. Это означает, что вам нужно обнулить градиенты на каждом шаге обучения, иначе вы сохраните градиенты от предыдущих обучающих пакетов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Изначальные веса - ', model[0].weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# Обнуляем градиенты, делаем это, потому что градиенты накапливаются\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Прямой проход, затем обратный проход, затем обновляем веса\n",
    "output = model(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Градиент -', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Делаем шаг обновления и видим новые веса\n",
    "optimizer.step()\n",
    "print('Обновленные веса - ', model[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение в цикле\n",
    "\n",
    "Теперь поместим этот алгоритм в цикл, чтобы пройти по всем изображениям. Один проход через весь набор данных называется *эпоха* (epoch). Здесь мы собираемся проитерироваться через `trainloader`, чтобы получить наши обучающие пакеты. Для каждого пакета мы сделаем обучение, где мы вычислим функцию потерь, выполним обратный проход и обновим веса.\n",
    "\n",
    "> **Упражнение:** Реализуйте обучение для нашей сети. Если вы реализовали его правильно, вы должны видеть, как функция потерь на обучающей выборке снижается с каждой эпохой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Разворачиваем изображения MNIST в вектор длиной 784\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # TODO: Обучающий шаг\n",
    " \n",
    "        loss = \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем проверить предсказания нашей обученной сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import helper\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Отключаем градиенты, чтобы ускорить эту часть\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "# Выход сети — это логарифмические вероятности, необходимо взять экспоненту для получения вероятностей\n",
    "ps = torch.exp(logps)\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь наша сеть может предсказывать цифры на изображениях MNIST. Далее вы напишете код для обучения нейронной сети на более сложном наборе данных."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
